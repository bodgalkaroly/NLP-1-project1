{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIZrAUx57vsM"
   },
   "source": [
    "Practical 1: Sentiment Detection in Movie Reviews\n",
    "========================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4kXPMhyngZW"
   },
   "source": [
    "This practical concerns detecting sentiment in movie reviews. This is a typical NLP classification task.\n",
    "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 positive and 1000 negative **movie reviews**.\n",
    "Each review is a **document** and consists of one or more sentences.\n",
    "\n",
    "To prepare yourself for this practical, you should\n",
    "have a look at a few of these texts to understand the difficulties of\n",
    "the task: how might one go about classifying the texts? You will write\n",
    "code that decides whether a movie review conveys positive or\n",
    "negative sentiment.\n",
    "\n",
    "Please make sure you have read the following paper:\n",
    "\n",
    ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
    "(2002).\n",
    "[Thumbs up? Sentiment Classification using Machine Learning\n",
    "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
    "\n",
    "Bo Pang et al. were the \"inventors\" of the movie review sentiment\n",
    "classification task, and the above paper was one of the first papers on\n",
    "the topic. The first version of your sentiment classifier will do\n",
    "something similar to Pang et al.'s system. If you have questions about it,\n",
    "you should resolve as soon as possible with your TA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb7errgRASzZ"
   },
   "source": [
    "**Advice**\n",
    "\n",
    "Please read through the entire practical and familiarise\n",
    "yourself with all requirements before you start coding or otherwise\n",
    "solving the tasks. Writing clean and concise code can make the difference\n",
    "between solving the assignment in a matter of hours, and taking days to\n",
    "run all experiments.\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "While we inserted code cells to indicate where you should implement your own code, please feel free to add/remove code blocks where you see fit (but make sure that the general structure of the assignment is preserved). Also, please keep in mind that it is always good practice to structure your code properly, e.g., by implementing separate classes and functions that can be reused.\n",
    "\n",
    "Generally, any function that we already use or import in the notebook can be used. Besides those, importing something like `deepcopy` or `tqdm` that does not change the implementation of the algorithms is fine. Functions that change or simplify the way you would implement the algorithm, including text processing functions from libraries like `sklearn`, `pandas` or `nltk` are not allowed unless specified (e.g. `ngrams` and `PorterStemmer` from `ntlk`). If you have questions about any specific function or library, please discuss this with your TA who will be the one grading your assignment.\n",
    "\n",
    "## Environment\n",
    "\n",
    "All code should be written in **Python 3**.\n",
    "This is the default in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "SaZnxptMJiD7",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.4\n"
     ]
    }
   ],
   "source": [
    "from IPython.utils import docs\n",
    "from sklearn.metrics import confusion_matrix\n",
    "!python --version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYZyIF7lJnGn"
   },
   "source": [
    "If you want to run code on your own computer, then download this notebook through `File -> Download .ipynb`.\n",
    "The easiest way to\n",
    "install Python is through downloading\n",
    "[Anaconda](https://www.anaconda.com/download).\n",
    "After installation, you can start the notebook by typing `jupyter notebook filename.ipynb`.\n",
    "You can also use an IDE\n",
    "such as [PyCharm](https://www.jetbrains.com/pycharm/download/) to make\n",
    "coding and debugging easier. It is good practice to create a [virtual\n",
    "environment](https://docs.python.org/3/tutorial/venv.html) for this\n",
    "project, so that any Python packages don’t interfere with other\n",
    "projects.\n",
    "\n",
    "\n",
    "**Learning Python 3**\n",
    "\n",
    "If you are new to Python 3, you may want to check out a few of these resources:\n",
    "- https://learnxinyminutes.com/docs/python3/\n",
    "- https://www.learnpython.org/\n",
    "- https://docs.python.org/3/tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hok-BFu9lGoK",
    "ExecuteTime": {
     "end_time": "2025-11-05T11:29:19.322967Z",
     "start_time": "2025-11-05T11:29:19.319148Z"
    }
   },
   "source": [
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "import typing\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import tqdm\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from scipy.sparse import csr_matrix"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXWyGHwE-ieQ"
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "**Download the sentiment lexicon and the movie reviews dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkPwuHp5LSuQ"
   },
   "source": [
    "**Load the movie reviews.**\n",
    "\n",
    "Each word in a review comes with its part-of-speech tag. For documentation on POS-tags, see https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "careEKj-mRpl",
    "ExecuteTime": {
     "end_time": "2025-11-05T11:29:23.645062Z",
     "start_time": "2025-11-05T11:29:22.528563Z"
    }
   },
   "source": [
    "# file structure:\n",
    "# [\n",
    "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
    "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
    "#   ..\n",
    "# ]\n",
    "# where `content` is a list of sentences,\n",
    "# with a sentence being a list of (token, pos_tag) pairs.\n",
    "\n",
    "\n",
    "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    reviews = json.load(f)\n",
    "\n",
    "print(\"Total number of reviews:\", len(reviews), \"\\n\")\n",
    "\n",
    "\n",
    "def print_sentence_with_pos(s):\n",
    "    print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
    "\n",
    "\n",
    "for i, r in enumerate(reviews):\n",
    "    print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
    "    print_sentence_with_pos(r[\"content\"][0])\n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "c = Counter()\n",
    "for review in reviews:\n",
    "    for sentence in review[\"content\"]:\n",
    "        for token, pos_tag in sentence:\n",
    "            c[token.lower()] += 1\n",
    "\n",
    "print(\"\\nNumber of word types:\", len(c))\n",
    "print(\"Number of word tokens:\", sum(c.values()))\n",
    "\n",
    "print(\"\\nMost common tokens:\")\n",
    "for token, count in c.most_common(20):\n",
    "    print(\"%10s : %8d\" % (token, count))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews: 2000 \n",
      "\n",
      "0 NEG 29\n",
      "Two/CD teen/JJ couples/NNS go/VBP to/TO a/DT church/NN party/NN ,/, drink/NN and/CC then/RB drive/NN ./.\n",
      "1 NEG 11\n",
      "Damn/JJ that/IN Y2K/CD bug/NN ./.\n",
      "2 NEG 24\n",
      "It/PRP is/VBZ movies/NNS like/IN these/DT that/WDT make/VBP a/DT jaded/JJ movie/NN viewer/NN thankful/JJ for/IN the/DT invention/NN of/IN the/DT Timex/NNP IndiGlo/NNP watch/NN ./.\n",
      "3 NEG 19\n",
      "QUEST/NN FOR/IN CAMELOT/NNP ``/`` Quest/NNP for/IN Camelot/NNP ''/'' is/VBZ Warner/NNP Bros./NNP '/POS first/JJ feature-length/JJ ,/, fully-animated/JJ attempt/NN to/TO steal/VB clout/NN from/IN Disney/NNP 's/POS cartoon/NN empire/NN ,/, but/CC the/DT mouse/NN has/VBZ no/DT reason/NN to/TO be/VB worried/VBN ./.\n",
      "4 NEG 38\n",
      "Synopsis/NNPS :/: A/DT mentally/RB unstable/JJ man/NN undergoing/VBG psychotherapy/NN saves/VBZ a/DT boy/NN from/IN a/DT potentially/RB fatal/JJ accident/NN and/CC then/RB falls/VBZ in/IN love/NN with/IN the/DT boy/NN 's/POS mother/NN ,/, a/DT fledgling/NN restauranteur/NN ./.\n",
      "\n",
      "Number of word types: 47743\n",
      "Number of word tokens: 1512359\n",
      "\n",
      "Most common tokens:\n",
      "         , :    77842\n",
      "       the :    75948\n",
      "         . :    59027\n",
      "         a :    37583\n",
      "       and :    35235\n",
      "        of :    33864\n",
      "        to :    31601\n",
      "        is :    25972\n",
      "        in :    21563\n",
      "        's :    18043\n",
      "        it :    15904\n",
      "      that :    15820\n",
      "     -rrb- :    11768\n",
      "     -lrb- :    11670\n",
      "        as :    11312\n",
      "      with :    10739\n",
      "       for :     9816\n",
      "       his :     9542\n",
      "      this :     9497\n",
      "      film :     9404\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6PWaEoh8B34"
   },
   "source": [
    "# Lexicon-based approach (4pts)\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T19:09:29.391059Z",
     "start_time": "2025-11-03T19:09:29.386112Z"
    },
    "id": "lm-rakqtlMOT"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 99,
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cwd_files = [f.name for f in Path.cwd().iterdir() if f.is_file()]\n",
    "\n",
    "if \"sent_lexicon\" not in cwd_files:\n",
    "    # download sentiment lexicon\n",
    "    !wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
    "if \"reviews.json\" not in cwd_files:\n",
    "    # download review data\n",
    "    !wget https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsTSMb6ma4E8"
   },
   "source": [
    "A traditional approach to automatically classify documents according to their sentiment is the lexicon-based approach. To implement this approach, you need a **sentiment lexicon**, i.e., a list of words annotated with a sentiment label (e.g., positive and negative) or a sentiment score (e.g., a score from 0 to 5).\n",
    "\n",
    "In this practical, you will use the sentiment\n",
    "lexicon released by Wilson et al. (2005). The path of the loaded lexicon is `\"sent_lexicon\"`.\n",
    "\n",
    "> Theresa Wilson, Janyce Wiebe, and Paul Hoffmann\n",
    "(2005). [Recognizing Contextual Polarity in Phrase-Level Sentiment\n",
    "Analysis](http://www.aclweb.org/anthology/H/H05/H05-1044.pdf). HLT-EMNLP.\n",
    "\n",
    "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (positive, negative, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon. For those lemmas you are free to use any entry, so it is fine to only keep one, e.g. the first or last one."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ogq0Eq2hQglh",
    "ExecuteTime": {
     "end_time": "2025-11-05T11:29:25.665088Z",
     "start_time": "2025-11-05T11:29:25.658419Z"
    }
   },
   "source": [
    "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    line_cnt = 0\n",
    "    for line in f:\n",
    "        print(line.strip())\n",
    "        line_cnt += 1\n",
    "        if line_cnt > 4:\n",
    "            break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
      "type=weaksubj len=1 word1=abandonment pos1=noun stemmed1=n priorpolarity=negative\n",
      "type=weaksubj len=1 word1=abandon pos1=verb stemmed1=y priorpolarity=negative\n",
      "type=strongsubj len=1 word1=abase pos1=verb stemmed1=y priorpolarity=negative\n",
      "type=strongsubj len=1 word1=abasement pos1=anypos stemmed1=y priorpolarity=negative\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mml4nOtIUBhn"
   },
   "source": [
    "Given such a sentiment lexicon, there are ways to solve\n",
    "the classification task without using Machine Learning. For example, one might look up every word $w_1 ... w_n$ in a document, and compute a **binary score**\n",
    "$S_{binary}$ by counting how many words have a positive or a\n",
    "negative label in the sentiment lexicon $SLex$.\n",
    "\n",
    "$$S_{binary}(w_1 w_2 ... w_n) = \\sum_{i = 1}^{n}\\text{sign}(SLex\\big[w_i\\big])$$\n",
    "\n",
    "where $\\text{sign}(SLex\\big[w_i\\big])$ refers to the polarity of $w_i$.\n",
    "\n",
    "**Threshold.** On average, there are more positive than negative words per review (~7.13 more positive than negative per review) to take this bias into account you should use a threshold of **8** (roughly the bias itself) to make it harder to classify as positive.\n",
    "\n",
    "$$\n",
    "\\text{classify}(S_{binary}(w_1 w_2 ... w_n)) = \\bigg\\{\\begin{array}{ll}\n",
    "        \\text{positive} & \\text{if } S_{binary}(w_1w_2...w_n) > threshold\\\\\n",
    "        \\text{negative} & \\text{otherwise}\n",
    "        \\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOFnMvbeeZrc"
   },
   "source": [
    "#### (Q1.1) Implement this classifier using the provided structure below. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pHmmMK91n80C",
    "ExecuteTime": {
     "end_time": "2025-11-05T11:29:27.965577Z",
     "start_time": "2025-11-05T11:29:27.956476Z"
    }
   },
   "source": [
    "class SentimentLexicon:\n",
    "    def __init__(\n",
    "        self,\n",
    "        lexicon_path: str = \"./sent_lexicon\",\n",
    "        threshold: float = 8.0,\n",
    "        weight: float = 1.0,\n",
    "    ) -> None:\n",
    "        \"\"\"Sentiment classifier using a lexicon approach.\n",
    "\n",
    "        Args:\n",
    "            lexicon_path (str, optional): the location of the sentiment lexicon file. Defaults to \"./sent_lexicon\".\n",
    "            threshold (float, optional): the threshold used in classification. Defaults to 8.0.\n",
    "            weight (float, optional): the weight assigned to words with strong polarity. Defaults to 1.0.\n",
    "        \"\"\"\n",
    "        self.lexicon_path = lexicon_path\n",
    "        self.threshold = threshold\n",
    "        self.weight = weight\n",
    "\n",
    "        self.lexicon: dict[str, dict[str, str]]\n",
    "\n",
    "        ##################\n",
    "        self.lexicon = {}\n",
    "        with open(self.lexicon_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()  # we split the text into individual lines\n",
    "                entries = {}\n",
    "                parts = line.split()\n",
    "                for entry in parts:\n",
    "                    if \"=\" in entry:\n",
    "                        key, value = entry.split(\"=\", 1)\n",
    "                        entries[key] = value\n",
    "\n",
    "                word = entries.get(\"word1\")  # we retrieve the individual dimensions\n",
    "                polarity = entries.get(\"priorpolarity\")\n",
    "                strength = entries.get(\"type\")\n",
    "\n",
    "                if not word or polarity in {\n",
    "                    \"neutral\",\n",
    "                    \"both\",\n",
    "                }:  # we dont store neutral or ambiguous words in the lexicon\n",
    "                    continue\n",
    "\n",
    "                # we encode score based on polarity\n",
    "                if polarity == \"positive\":\n",
    "                    score = 1\n",
    "                else:  # for negative words\n",
    "                    score = -1\n",
    "                if strength == \"strongsubj\":\n",
    "                    score *= self.weight  # we adjust the weight\n",
    "                # we store the words\n",
    "                self.lexicon[word.lower()] = score  # we convert to words to their lower case format\n",
    "        ##################\n",
    "\n",
    "    def fit(self, documents: list[list[str]], labels: list[typing.Literal[\"POS\", \"NEG\"]]) -> None:\n",
    "        \"\"\"Fit the classifer according to the input features and target labels.\n",
    "\n",
    "        For the `SentimentLexicon` classifier, there are no parameters to learn.\n",
    "\n",
    "        Args:\n",
    "            documents (list[list[str]]): the list of documents as a list of tokens.\n",
    "            labels (list[str]): the list of labels\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, documents: list[list[str]]) -> list[typing.Literal[\"POS\", \"NEG\"]]:\n",
    "        \"\"\"Perform classification on input documents.\n",
    "\n",
    "        Args:\n",
    "            documents (list[list[str]]): the list of documents as a list of tokens.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: predicted labels.\n",
    "        \"\"\"\n",
    "        ##################\n",
    "        predictions = []\n",
    "        for doc in tqdm.tqdm(documents):\n",
    "            total_score = 0\n",
    "            for word in doc:\n",
    "                word = word.lower()  # we convert to lower case\n",
    "                if word in self.lexicon:\n",
    "                    total_score += self.lexicon[word]\n",
    "                else:\n",
    "                    continue\n",
    "            if total_score > self.threshold:\n",
    "                predictions.append(\"POS\")\n",
    "            else:\n",
    "                predictions.append(\"NEG\")\n",
    "        return predictions\n",
    "        ##################"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YryJZo_5n80C"
   },
   "source": [
    "#### (Q1.2) Implement functions to transform the dataset to the needed data structures. (0.5pt)\n",
    "\n",
    "In our current implementation, the classifier's `fit` and `predict` methods expect a list of corpus documents represented as `list[list[str]]`, and labels as a `list[str]`. However, our data is represented as `list[dict[str, Any]]`. Before we can fit and classify, we'll need to define a function to transform our data to the desired data structure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R96Ep9R-n80D",
    "ExecuteTime": {
     "end_time": "2025-11-05T11:29:29.917944Z",
     "start_time": "2025-11-05T11:29:29.913218Z"
    }
   },
   "source": [
    "def extract_labels(\n",
    "    documents: list[dict[str, typing.Any]],\n",
    ") -> list[typing.Literal[\"POS\", \"NEG\"]]:\n",
    "    \"\"\"Converts a list of reviews to a list of labels.\n",
    "\n",
    "    Args:\n",
    "        documents (list[dict[str, Any]]): the reviews as a list of dicts.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: the labels as a list of str\n",
    "    \"\"\"\n",
    "    ##################\n",
    "    labels = []\n",
    "    for doc in documents:\n",
    "        labels.append(doc[\"sentiment\"])\n",
    "    return labels\n",
    "    ##################\n",
    "\n",
    "\n",
    "def extract_unigrams(documents: list[dict[str, typing.Any]], lower: bool = True) -> list[list[str]]:\n",
    "    \"\"\"Converts a list of reviews to a list of unigram token lists.\n",
    "\n",
    "    Args:\n",
    "        documents (list[dict[str, typing.Any]]): the reviews as a list of dicts.\n",
    "        lower (bool, optional): whether to lowercase the review tokens. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[list[str]]: the list of unigram tokens\n",
    "    \"\"\"\n",
    "    ##################\n",
    "    all_unigrams = []  # we initialize a list collecting asll unigrams in all documents\n",
    "    for doc in documents:\n",
    "        unigrams_per_doc = (\n",
    "            []\n",
    "        )  # we intialize a list for the unigrams appearing in the specific document\n",
    "        for sentence in doc[\"content\"]:  # we loop over the sentences in the individual docs\n",
    "            for word, pos in sentence:\n",
    "                if lower == True:  # if we want to lowercase the review tokens\n",
    "                    word = word.lower()\n",
    "                unigrams_per_doc.append(word)\n",
    "        all_unigrams.append(unigrams_per_doc)\n",
    "    return all_unigrams\n",
    "    ##################"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2lyr9bkn80D"
   },
   "source": [
    "#### (Q1.3) Implement a function to compute the accuracy of a classification. (0.5pt)\n",
    "\n",
    "Finally, we need to implement a classification evaluation metric. For this practical, we'll be using [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_classification), which is defined as the proportion of correct classifications."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8_vxDC0sn80D",
    "ExecuteTime": {
     "end_time": "2025-11-05T11:29:32.711704Z",
     "start_time": "2025-11-05T11:29:32.707486Z"
    }
   },
   "source": [
    "def accuracy(y_pred: list[str], y_true: list[str]) -> float:\n",
    "    \"\"\"Computes the accuracy score.\n",
    "\n",
    "    Args:\n",
    "        y_pred (list[str]): the predicted labels\n",
    "        y_true (list[str]): the ground truth labels\n",
    "\n",
    "    Returns:\n",
    "        float: the accuracy score\n",
    "    \"\"\"\n",
    "    ##################\n",
    "    true_positives = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            true_positives += 1\n",
    "    accuracy = true_positives / len(y_pred)\n",
    "    return accuracy\n",
    "    ##################"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBEgsK3nn80D"
   },
   "source": [
    "Now train, predict and evaluate your classifier using the implemented functions. Make sure to print the final accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XPyv_fvVn80D",
    "ExecuteTime": {
     "end_time": "2025-11-05T11:29:34.770824Z",
     "start_time": "2025-11-05T11:29:34.359184Z"
    }
   },
   "source": [
    "##################\n",
    "# First, we extract the labels and tokens\n",
    "labels = extract_labels(reviews)\n",
    "tokens = extract_unigrams(reviews)\n",
    "\n",
    "# Second, we initialize our classifier\n",
    "classifier = SentimentLexicon(\n",
    "    lexicon_path=\"./sent_lexicon\", threshold=8.0, weight=1.0\n",
    ")  # with the predefined threshold, and basline weight\n",
    "\n",
    "# For third step, we could fit the classifier, but since we dont have learnable parameters, we skip this step\n",
    "# Now, we predict sentiment for the documents\n",
    "predictions = classifier.predict(tokens)\n",
    "\n",
    "# At last, we derive accuracy\n",
    "performance = accuracy(predictions, labels)\n",
    "\n",
    "print(f\"The accuracy of the Lexicon-based classifier on our dataset is {performance}.\")\n",
    "##################"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 14517.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Lexicon-based classifier on our dataset is 0.6785.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Twox0s_3eS0V"
   },
   "source": [
    "#### (Q1.4) Now incorporate magnitude information and report the classification accuracy. Don't forget to use the threshold. (1 pt)\n",
    "\n",
    "As the sentiment lexicon also has information about the **magnitude** of\n",
    "sentiment (e.g., *“excellent\"* would have higher magnitude than\n",
    "*“good\"*), we can take a more fine-grained approach by adding up all\n",
    "sentiment scores, and deciding the polarity of the movie review using\n",
    "the sign of the weighted score $S_{weighted}$.\n",
    "\n",
    "$$S_{weighted}(w_1w_2...w_n) = \\sum_{i = 1}^{n}SLex\\big[w_i\\big]$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9vVk7CvDpyka",
    "ExecuteTime": {
     "end_time": "2025-11-05T11:29:36.673136Z",
     "start_time": "2025-11-05T11:29:36.520720Z"
    }
   },
   "source": [
    "# Second, we initialize our classifier with a weight for magnitude\n",
    "classifier_weighted = SentimentLexicon(\n",
    "    lexicon_path=\"./sent_lexicon\", threshold=8.0, weight=2.0\n",
    ")  # with the predefined threshold, and adjusted higher weight\n",
    "\n",
    "# For third step, we could fit the classifier, but since we don't have learnable parameters, we skip this step\n",
    "# Now, we predict sentiment for the documents\n",
    "predictions = classifier_weighted.predict(tokens)\n",
    "\n",
    "# At last, we derive accuracy\n",
    "performance_weighted = accuracy(predictions, labels)\n",
    "\n",
    "print(\n",
    "    \"The accuracy of the weighted Lexicon-based classifier on our dataset is\"\n",
    "    f\" {performance_weighted}.\"\n",
    ")\n",
    "##################"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 15281.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the weighted Lexicon-based classifier on our dataset is 0.6865.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNhS8OCVxMHd"
   },
   "source": [
    "#### (Q1.5) A better threshold (1pt)\n",
    "\n",
    "Above we have defined a threshold to account for an inherent bias in the dataset: there are more positive than negative words per review.\n",
    "However, that threshold does not take into account *document length*. Explain why this is a problem and implement an alternative way to compute the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_uSlPv2n80E"
   },
   "source": [
    "\\####################\n",
    "The underlying assumption is, that the document lengths are fairly similar. The observed bias is, that the proportion of negative/postive words is fairly constant, with a higher bias towards positive words. When the document length is uniform, and hence the number of positve word occurences is relatively uniform too, then this bias can be expressed as an average of 8 more positive word occurances per document. However, with the document length varying (and hence the number of positive words varying), the proportion between the negative/postive words is constant, not their difference. While for documents with 16 positive words we expect 8 negative word (the proportion is 0.5), for a document with 32 positive words we would not expect 24 negative words, but rather 16 (the proportion of 0.5 should be constant, not the difference of 8).\n",
    "Consequently, we need to adopt a threshold, which takes into account the varying document length/number of sentiment word occurances, to succesfully model the constant proportion instead of the difference)\n",
    "\n",
    "\\####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7n9DpkyYn80E"
   },
   "source": [
    "Now implement your agorithm by adjusting the previously created `SentimentLexicon` class. Make sure to print the final accuracy score and compare to the previous results.\n",
    "\n",
    "Note: you might need to adjust your threshold."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xxnBsTxnn80E",
    "ExecuteTime": {
     "end_time": "2025-11-05T11:29:39.254350Z",
     "start_time": "2025-11-05T11:29:39.247226Z"
    }
   },
   "source": [
    "class BetterSentimentLexicon(SentimentLexicon):\n",
    "    def predict(self, documents: list[list[str]]) -> list[str]:\n",
    "        \"\"\"Perform classification on input documents.\n",
    "\n",
    "        Args:\n",
    "            documents (list[list[str]]): the list of documents as a list of tokens.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: predicted labels.\n",
    "        \"\"\"\n",
    "        ##################\n",
    "        predictions = []\n",
    "        for doc in tqdm.tqdm(documents):\n",
    "            total_score = 0\n",
    "            for word in doc:\n",
    "                word = word.lower()  # we convert to lower case\n",
    "                if word in self.lexicon:\n",
    "                    total_score += self.lexicon[word]\n",
    "                else:\n",
    "                    continue\n",
    "            # we normalize the total score, to incorporate document length\n",
    "            normalized_score = total_score / len(doc)\n",
    "            if (\n",
    "                normalized_score > self.threshold\n",
    "            ):  # in this case, threshold should be a float, incorporating the proportion in bias\n",
    "                predictions.append(\"POS\")\n",
    "            else:\n",
    "                predictions.append(\"NEG\")\n",
    "        return predictions\n",
    "        ##################"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JBvPGZKan80E",
    "ExecuteTime": {
     "end_time": "2025-11-05T11:29:41.226733Z",
     "start_time": "2025-11-05T11:29:41.069618Z"
    }
   },
   "source": [
    "##################\n",
    "\n",
    "# Second, we initialize our classifier with a weight for magnitude\n",
    "classifier_thresh = BetterSentimentLexicon(\n",
    "    lexicon_path=\"./sent_lexicon\", threshold=0.02, weight=2.0\n",
    ")  # with the adjusted threshold depicting bias in terms of proportion, and adjusted higher weight\n",
    "\n",
    "# For third step, we could fit the classifier, but since we don't have learnable parameters, we skip this step\n",
    "# Now, we predict sentiment for the documents\n",
    "predictions = classifier_thresh.predict(tokens)\n",
    "\n",
    "# At last, we derive accuracy\n",
    "performance_thresh = accuracy(predictions, labels)\n",
    "\n",
    "print(\n",
    "    f\"The accuracy of the weighted Lexicon-based classifier on our dataset is {performance_thresh}.\"\n",
    ")\n",
    "##################\n",
    "\n",
    "##################"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 14793.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the weighted Lexicon-based classifier on our dataset is 0.69.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnF9adQnuwia"
   },
   "source": [
    "# Naive Bayes (10pt)\n",
    "\n",
    "Your second task is to program a simple Machine Learning approach that operates\n",
    "on a simple Bag-of-Words (BoW) representation of the text data, as\n",
    "described by Pang et al. (2002). In this approach, the only features we\n",
    "will consider are the words in the text themselves, without bringing in\n",
    "external sources of information. The BoW model is a popular way of\n",
    "representing texts as vectors, making it\n",
    "easy to apply classical Machine Learning algorithms on NLP tasks.\n",
    "However, the BoW representation is also very crude, since it discards\n",
    "all information related to word order and grammatical structure in the\n",
    "original text—as the name suggests.\n",
    "\n",
    "## Writing your own classifier (4pts)\n",
    "\n",
    "Write your own code to implement the Naive Bayes (NB) classifier. As\n",
    "a reminder, the Naive Bayes classifier works according to the following\n",
    "equation:\n",
    "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} P(c|\\bar{f}) = \\operatorname*{arg\\,max}_{c \\in C} P(c)\\prod^n_{i=1} P(f_i|c)$$\n",
    "where $C = \\{ \\text{POS}, \\text{NEG} \\}$ is the set of possible classes,\n",
    "$\\hat{c} \\in C$ is the most probable class, and $\\bar{f}$ is the feature\n",
    "vector. Remember that we use the log of these probabilities when making\n",
    "a prediction:\n",
    "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} \\Big\\{\\log P(c) + \\sum^n_{i=1} \\log P(f_i|c)\\Big\\}$$\n",
    "\n",
    "You can find more details about Naive Bayes in [Jurafsky &\n",
    "Martin](https://web.stanford.edu/~jurafsky/slp3/). You can also look at\n",
    "this helpful\n",
    "[pseudo-code](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
    "\n",
    "*Note: this section and the next aim to put you in a position to replicate\n",
    "    Pang et al.'s Naive Bayes results. However, your numerical results\n",
    "    will differ from theirs, as they used different data.*\n",
    "\n",
    "**You must write the Naive Bayes training and prediction code from\n",
    "scratch.** You will not be given credit for using off-the-shelf Machine\n",
    "Learning libraries.\n",
    "\n",
    "The data contains the text of the reviews, where each document consists\n",
    "of the sentences in the review, the sentiment of the review and an index\n",
    "(cv) that you will later use for cross-validation. The\n",
    "text has already been tokenised and POS-tagged for you. Your algorithm\n",
    "should read in the text, **lowercase it**, store the words and their\n",
    "frequencies in an appropriate data structure that allows for easy\n",
    "computation of the probabilities used in the Naive Bayes algorithm, and\n",
    "then make predictions for new instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEpyQSBSkb33"
   },
   "source": [
    "#### (Q2.1) Unseen words (1pt)\n",
    "The presence of words in the test dataset that\n",
    "have not been seen during training can cause probabilities in the Naive Bayes classifier to equal $0$.\n",
    "These can be words which are unseen in both positive and negative training reviews (case 1), but also words which are seen in reviews _of only one sentiment class_ in the training dataset (case 2). In both cases, **you should skip these words for both classes at test time**.  What would be the problem instead with skipping words only for one class in case 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BanFiYYnoxDW"
   },
   "source": [
    "\\####################\n",
    "\n",
    "The problem with skipping words for only one class in case 2 is that we would introduce a significant negative bias in our classifier for one class. First, by skipping the term \"A\" for Class 1 (for which the word \"A\" did not appear in the training data), the equation would miss the term log(P(A| Class 1)) for c=Class 1. However, without skipping it for Class 2, the equation would include the term log(P(A| Class 2)) for c=Class 2. Since 0<P(A|Class 2)=<1, log(P(A|CLass 2)=<0, when calculating\n",
    "\n",
    "\n",
    "$$\\Big\\{\\log P(Class 2) + \\sum^n_{i=1} \\log P(f_i|Class 2)\\Big\\}$$,\n",
    "\n",
    "the equation would have an additional negative term for log(P(A|Class 2)), introducing a negative bias against Class 2. This bias is very counterintuitive, since the model is penalizing the class which had the evidence for a word occurence in contrast to the other class, that did not have that word occurance at all. Hence, to mitigate this negative and counterintuitive bias, the best approach is to skip this word A for both classes.\n",
    "\n",
    "\\####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsZRhaI3WvzC"
   },
   "source": [
    "#### (Q2.2) Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining (positive and negative) reviews cv900–cv999.  Report results using classification accuracy as your evaluation metric. (2pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RUMXCeRTn80F",
    "ExecuteTime": {
     "end_time": "2025-11-05T11:29:46.502069Z",
     "start_time": "2025-11-05T11:29:46.492530Z"
    }
   },
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, smoothing: float = 0):\n",
    "        \"\"\"Naive Bayes classifier for multinomial data.\n",
    "\n",
    "        Args:\n",
    "            smoothing (float, optional): the Laplacian smoothing factor. Defaults to 0.\n",
    "        \"\"\"\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "        # The datastructures necessary for NaiveBayes\n",
    "        self.vocab: set[str] = set()\n",
    "\n",
    "        self.prior: dict[typing.Literal[\"POS\", \"NEG\"], float] = {\n",
    "            \"POS\": 0,\n",
    "            \"NEG\": 0,\n",
    "        }\n",
    "\n",
    "        self.token_counts: dict[str, dict[typing.Literal[\"POS\", \"NEG\"], int]] = defaultdict(\n",
    "            lambda: {\"POS\": 0, \"NEG\": 0},\n",
    "        )\n",
    "\n",
    "        self.cond_prob: dict[str, dict[typing.Literal[\"POS\", \"NEG\"], float]] = defaultdict(\n",
    "            lambda: {\"POS\": 0.0, \"NEG\": 0.0},\n",
    "        )\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        documents: list[list[typing.Any]],\n",
    "        labels: list[typing.Literal[\"POS\", \"NEG\"]],\n",
    "    ) -> None:\n",
    "        \"\"\"Fit the classifer according to the input features and target labels.\n",
    "\n",
    "        Args:\n",
    "            documents (list[list[str]]): the list of documents as a list of tokens\n",
    "            labels (list[str]): the list of labels\n",
    "        \"\"\"\n",
    "        ##################\n",
    "        # First, we start by estimating the priors\n",
    "        nr_documents = len(documents)\n",
    "        nr_pos_doc = sum(1 for label in labels if label == \"POS\")\n",
    "        nr_neg_doc = nr_documents - nr_pos_doc\n",
    "\n",
    "        self.prior[\"POS\"] = math.log(\n",
    "            nr_pos_doc / nr_documents\n",
    "        )  # the log prior for the positive and negative classes\n",
    "        self.prior[\"NEG\"] = math.log(nr_neg_doc / nr_documents)\n",
    "\n",
    "        # Second, we estimate the conditional probabilities\n",
    "        for words, label in zip(documents, labels):  # we start by creating the vocabulary\n",
    "            for word in words:\n",
    "                # Handle both strings and tuples (n-grams) for exercise 2.8\n",
    "                if isinstance(word, str):\n",
    "                    word = word.lower()\n",
    "                # For tuples (n-grams), they are already lowercased in extract_ngrams\n",
    "\n",
    "                self.vocab.add(word)\n",
    "                self.token_counts[word][label] += 1\n",
    "\n",
    "        # to derive the class conditional probailities, we calculate the total word counts for each class\n",
    "        self.total_word_class = {\"POS\": 0, \"NEG\": 0}\n",
    "        for word in self.vocab:\n",
    "            self.total_word_class[\"POS\"] += self.token_counts[word][\"POS\"]\n",
    "            self.total_word_class[\"NEG\"] += self.token_counts[word][\"NEG\"]\n",
    "        # we compute the conditional probabilites with smoothing\n",
    "        for word in self.vocab:\n",
    "            for clss in [\"POS\", \"NEG\"]:\n",
    "                numerator = self.token_counts[word][clss] + self.smoothing\n",
    "                denominator = self.total_word_class[clss] + self.smoothing * len(self.vocab)\n",
    "                self.cond_prob[word][clss] = (\n",
    "                    numerator / denominator\n",
    "                )  # conditional probabilities per word\n",
    "\n",
    "        ##################\n",
    "\n",
    "    def predict(self, documents: list[list[typing.Any]]) -> list[typing.Literal[\"POS\", \"NEG\"]]:\n",
    "        \"\"\"Perform classification on input documents.\n",
    "\n",
    "        Args:\n",
    "            documents (list[list[str]]): the list of documents as a list of tokens\n",
    "\n",
    "        Returns:\n",
    "            list[str]: predicted labels\n",
    "        \"\"\"\n",
    "        ##################\n",
    "        # we initialize the predicted labels as an empty list\n",
    "        pred_labels = []\n",
    "        # We start by iterating over every document\n",
    "        for document in documents:\n",
    "            c_hat_pos = self.prior[\"POS\"]\n",
    "            c_hat_neg = self.prior[\"NEG\"]\n",
    "            for word in document:\n",
    "                # Handle both strings and tuples (n-grams) for exercise 2.8\n",
    "                if isinstance(word, str):\n",
    "                    word = word.lower()\n",
    "                # For tuples (n-grams), they are already lowercased in extract_ngrams\n",
    "\n",
    "                if word not in self.vocab:  # we skip unseen words\n",
    "                    continue\n",
    "                if (\n",
    "                    self.token_counts[word][\"POS\"] == 0 or self.token_counts[word][\"NEG\"] == 0\n",
    "                ):  # we skip words only represented in one class\n",
    "                    continue\n",
    "                c_hat_pos += math.log(self.cond_prob[word][\"POS\"])\n",
    "                c_hat_neg += math.log(self.cond_prob[word][\"NEG\"])\n",
    "            if c_hat_pos > c_hat_neg:\n",
    "                pred_labels.append(\"POS\")\n",
    "            else:\n",
    "                pred_labels.append(\"NEG\")\n",
    "        return pred_labels\n",
    "        ##################"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAc3OKHxn80F"
   },
   "source": [
    "Now train, predict and evaluate your classifier on the reviews dataset. Make sure to select the right reviews for the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XKKRBL2Rn80F",
    "ExecuteTime": {
     "end_time": "2025-11-05T11:29:49.538026Z",
     "start_time": "2025-11-05T11:29:48.797747Z"
    }
   },
   "source": [
    "##################\n",
    "\n",
    "# we start by splitting our training data and test data\n",
    "train_docs, train_labels = [], []\n",
    "test_docs, test_labels = [], []\n",
    "for doc, label in zip(reviews, labels):\n",
    "    if doc[\"cv\"] >= 0 and doc[\"cv\"] <= 899:\n",
    "        train_docs.append(doc)\n",
    "        train_labels.append(label)\n",
    "    elif doc[\"cv\"] >= 900 and doc[\"cv\"] <= 999:\n",
    "        test_docs.append(doc)\n",
    "        test_labels.append(label)\n",
    "\n",
    "# now we extract the training and test tokens from the review dataset with the method introudced in part 1\n",
    "train_tokens = extract_unigrams(train_docs)\n",
    "test_tokens = extract_unigrams(test_docs)\n",
    "\n",
    "# we initialize the classifier without smoothing\n",
    "nb_classifier = NaiveBayes(smoothing=0)\n",
    "\n",
    "# we fit it\n",
    "nb_classifier.fit(train_tokens, train_labels)\n",
    "\n",
    "# we derive the predictions\n",
    "predictions = nb_classifier.predict(test_tokens)\n",
    "\n",
    "# at last, we derive the accuracy with our function used previously\n",
    "performance = accuracy(predictions, test_labels)\n",
    "print(\n",
    "    \"The performance of the Naive Bayes classifier on our dataset with Laplacian smoothing of\"\n",
    "    f\" {nb_classifier.smoothing} is {performance}.\"\n",
    ")\n",
    "##################"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of the Naive Bayes classifier on our dataset with Laplacian smoothing of 0 is 0.825.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0INK-PBoM6CB"
   },
   "source": [
    "#### (Q2.3) Would you consider accuracy to also be a good way to evaluate your classifier in a situation where 90% of your data instances are of positive movie reviews? (1pt)\n",
    "\n",
    "Simulate this scenario by keeping the positive reviews\n",
    "data unchanged, but only using negative reviews cv000–cv089 for\n",
    "training, and cv900–cv909 for testing. Calculate the classification\n",
    "accuracy, and explain what changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFbcsYlipBAw"
   },
   "source": [
    "\\####################\n",
    "\n",
    "The accuracy metric is inherently a bad choice for evaluating classifiers on a skewed dataset with minority class. In our case, this minority class is Negative reviews. The theoretical argument against using accuracy as a describing metric in skewed datasets is that a trivial classifier that always predicts the majority class (in this case POS), would get an accuracy of the proportion of the majority class. In this case, a classifier predicting POS all the time would have a 90% accuracy evaluated on the skewed dataset. However, this model would not perform well on a novel dataset.\n",
    "\n",
    "However, when I ran the experiment, the accuracy of the classifier dropped to 60 % from the intial 82.5%. this can be explained by the training process, and by taking a closer look at the confusion matrix of the predictions. We can infer, that the classifier did not predict only one class,but for the negative class it predicted negative class 90% of the time, and for positive class the accuracy was much lower around 57%. These results imply that the model overfit on the minority class, and failed to generalize to the majority class \"POS\". Hence, accuracy is not a good metric to describe model performance for skewed datasets, but for meaningful evaluations we need to include specific metrics, such as Precision, Recall as well.\n",
    "\\####################"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9zmChKqnn80F",
    "ExecuteTime": {
     "end_time": "2025-11-05T11:29:51.784928Z",
     "start_time": "2025-11-05T11:29:51.398234Z"
    }
   },
   "source": [
    "##################\n",
    "# we start by splitting our training data and test data\n",
    "train_docs, train_labels = [], []\n",
    "test_docs, test_labels = [], []\n",
    "for doc, label in zip(reviews, labels):\n",
    "    if label == \"POS\":\n",
    "        if doc[\"cv\"] >= 0 and doc[\"cv\"] <= 899:\n",
    "            train_docs.append(doc)\n",
    "            train_labels.append(label)\n",
    "        elif doc[\"cv\"] >= 900 and doc[\"cv\"] <= 999:\n",
    "            test_docs.append(doc)\n",
    "            test_labels.append(label)\n",
    "    elif label == \"NEG\":\n",
    "        if doc[\"cv\"] >= 0 and doc[\"cv\"] <= 89:\n",
    "            train_docs.append(doc)\n",
    "            train_labels.append(label)\n",
    "        elif doc[\"cv\"] >= 900 and doc[\"cv\"] <= 909:\n",
    "            test_docs.append(doc)\n",
    "            test_labels.append(label)\n",
    "# now we extract the training and test tokens from the review dataset with the method introudced in part 1\n",
    "train_tokens = extract_unigrams(train_docs)\n",
    "test_tokens = extract_unigrams(test_docs)\n",
    "\n",
    "# we initialize the classifier without smoothing\n",
    "nb_classifier = NaiveBayes(smoothing=0)\n",
    "\n",
    "# we fit it\n",
    "nb_classifier.fit(train_tokens, train_labels)\n",
    "\n",
    "# we derive the predictions\n",
    "predictions = nb_classifier.predict(test_tokens)\n",
    "\n",
    "# at last, we derive the accuracy with our function used previously\n",
    "performance = accuracy(predictions, test_labels)\n",
    "print(\n",
    "    \"The performance of the Naive Bayes classifier on our skewed dataset with Laplacian smoothing\"\n",
    "    f\" of {nb_classifier.smoothing} is {performance}.\"\n",
    ")\n",
    "\n",
    "##################"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of the Naive Bayes classifier on our skewed dataset with Laplacian smoothing of 0 is 0.6.\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T11:35:49.856760Z",
     "start_time": "2025-11-05T11:35:49.850887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for deeper analysis, we look at the confusion matrix as well\n",
    "def confusion_matrix(y_true, y_pred, labels=(\"POS\", \"NEG\")):\n",
    "    cm = {a: {b: 0 for b in labels} for a in labels}\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[t][p] += 1\n",
    "    return cm\n",
    "\n",
    "cm = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "# We print it in readable format\n",
    "print(\"                    PREDICTED\")\n",
    "print(\"              POS        NEG\")\n",
    "print(\"GROUND TRUTH --------------------\")\n",
    "for true_label in [\"POS\", \"NEG\"]:\n",
    "    row = f\"{true_label:11s}\"\n",
    "    for pred_label in [\"POS\", \"NEG\"]:\n",
    "        row += f\"{cm[true_label][pred_label]:10d}\"\n",
    "    print(row)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    PREDICTED\n",
      "              POS        NEG\n",
      "GROUND TRUTH --------------------\n",
      "POS                57        43\n",
      "NEG                 1         9\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wJzcHX3WUDm"
   },
   "source": [
    "## Smoothing (1pt)\n",
    "\n",
    "As mentioned above, the presence of words in the test dataset that\n",
    "have not been seen during training can cause probabilities in the Naive\n",
    "Bayes classifier to be $0$, thus making that particular test instance\n",
    "undecidable. The standard way to mitigate this effect (as well as to\n",
    "give more clout to rare words) is to use smoothing, in which the\n",
    "probability fraction\n",
    "\n",
    "$$\\frac{\\text{count}(w_i, c)}{\\sum\\limits_{w\\in V} \\text{count}(w, c)}$$\n",
    "\n",
    "for a word $w_i$ becomes\n",
    "\n",
    "$$\\frac{\\text{count}(w_i, c) + \\text{smoothing}(w_i)}{\\sum\\limits_{w\\in V} \\text{count}(w, c) + \\sum\\limits_{w \\in V} \\text{smoothing}(w)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBNIcbwUWphC"
   },
   "source": [
    "#### (Q2.4) Implement Laplace feature smoothing (1pt)\n",
    "Implement Laplace smoothing, i.e., smoothing with a constant value ($smoothing(w) = \\kappa, \\forall w \\in V$), in your Naive\n",
    "Bayes classifier’s code, and report the accuracy for $\\kappa = 1$ and $\\kappa = 3.5$. Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining (positive and negative) reviews cv900–cv999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T20:50:01.612380Z",
     "start_time": "2025-11-03T20:50:00.530049Z"
    },
    "id": "g03yflCc9kpW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of the Naive Bayes classifier on our dataset with Laplacian smoothing of 1 is 0.835.\n",
      "\n",
      "\n",
      "The performance of the Naive Bayes classifier on our dataset with Laplacian smoothing of 3.5 is 0.835.\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "\n",
    "# we start by splitting our training data and test data\n",
    "train_docs, train_labels = [], []\n",
    "test_docs, test_labels = [], []\n",
    "for doc, label in zip(reviews, labels):\n",
    "    if doc[\"cv\"] >= 0 and doc[\"cv\"] <= 899:\n",
    "        train_docs.append(doc)\n",
    "        train_labels.append(label)\n",
    "    elif doc[\"cv\"] >= 900 and doc[\"cv\"] <= 999:\n",
    "        test_docs.append(doc)\n",
    "        test_labels.append(label)\n",
    "\n",
    "# now we extract the training and test tokens from the review dataset with the method introudced in part 1\n",
    "train_tokens = extract_unigrams(train_docs)\n",
    "test_tokens = extract_unigrams(test_docs)\n",
    "\n",
    "# we initialize the classifier without smoothing\n",
    "nb_classifier1 = NaiveBayes(smoothing=1)  # k=1\n",
    "nb_classifier2 = NaiveBayes(smoothing=3.5)  # k=3.5\n",
    "\n",
    "# we fit it\n",
    "nb_classifier1.fit(train_tokens, train_labels)\n",
    "nb_classifier2.fit(train_tokens, train_labels)\n",
    "\n",
    "# we derive the predictions\n",
    "predictions1 = nb_classifier1.predict(test_tokens)\n",
    "predictions2 = nb_classifier2.predict(test_tokens)\n",
    "\n",
    "# at last, we derive the accuracy with our function used previously\n",
    "performance1 = accuracy(predictions1, test_labels)\n",
    "performance2 = accuracy(predictions2, test_labels)\n",
    "\n",
    "print(\n",
    "    \"The performance of the Naive Bayes classifier on our dataset with Laplacian smoothing of\"\n",
    "    f\" {nb_classifier1.smoothing} is {performance1}.\"\n",
    ")\n",
    "print(\n",
    "    \"\\n\\nThe performance of the Naive Bayes classifier on our dataset with Laplacian smoothing of\"\n",
    "    f\" {nb_classifier2.smoothing} is {performance2}.\"\n",
    ")\n",
    "\n",
    "\n",
    "##################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXF3oGVwn80G"
   },
   "source": [
    "**FROM NOW ON, ALWAYS USE SMOOTHING (YOU CAN CHOOSE $\\kappa$) WHEN USING THE NAIVE BAYES CLASSIFIER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiGcgwba87D5"
   },
   "source": [
    "## Cross-Validation (1pt)\n",
    "\n",
    "A serious danger in using Machine Learning on small datasets, with many\n",
    "iterations of slightly different versions of the algorithms, is ending up with Type III errors, also called the “testing hypotheses\n",
    "suggested by the data” errors. This type of error occurs when we make\n",
    "repeated improvements to our classifiers by playing with features and\n",
    "their processing, but we don’t get a fresh, never-before seen test\n",
    "dataset every time. Thus, we risk developing a classifier that gets better\n",
    "and better on our data, but only gets worse at generalizing to new, unseen data. In other words, we risk developping a classifier that overfits.\n",
    "\n",
    "A simple method to guard against Type III errors is to use\n",
    "Cross-Validation. In **N-fold Cross-Validation**, we divide the data into N\n",
    "distinct chunks, or folds. Then, we repeat the experiment N times: each\n",
    "time holding out one of the folds for testing, training our classifier\n",
    "on the remaining N - 1 data folds, and reporting performance on the\n",
    "held-out fold. We can use different strategies for dividing the data:\n",
    "\n",
    "-   Consecutive splitting:\n",
    "  - cv000–cv099 = Split 1\n",
    "  - cv100–cv199 = Split 2\n",
    "  - etc.\n",
    "  \n",
    "-   Round-robin splitting (mod 10):\n",
    "  - cv000, cv010, cv020, … = Split 1\n",
    "  - cv001, cv011, cv021, … = Split 2\n",
    "  - etc.\n",
    "\n",
    "-   Random sampling/splitting\n",
    "  - Not used here (but you may choose to split this way in a non-educational situation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OeLcbSauGtR"
   },
   "source": [
    "#### (Q2.5) Write the code to implement 10-fold cross-validation using round-robin splitting for your Naive Bayes classifier from Q2.4 and compute the 10 accuracies. Report the mean and standard deviation of the accuracies. (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKhv5-uRn80H"
   },
   "outputs": [],
   "source": [
    "def cross_validation_splits(\n",
    "    documents: list[dict[str, typing.Any]],\n",
    "    num_folds: int = 10,\n",
    ") -> list[tuple[list, list]]:\n",
    "    \"\"\"Splits the reviews into disjoint subsets of train/test splits.\n",
    "\n",
    "    Args:\n",
    "        documents (list[dict[str, typing.Any]]): the reviews as dicts.\n",
    "        num_folds (int, optional): the number of cross-validation folds. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[list, list]]: a list of train/test folds\n",
    "    \"\"\"\n",
    "    ##################\n",
    "    # Initialize list to hold all folds (train, test) pairs\n",
    "    folds = []\n",
    "\n",
    "    # For each fold, we assign which documents are test and which are train\n",
    "    for fold_idx in range(num_folds):\n",
    "        train_fold = []\n",
    "        test_fold = []\n",
    "\n",
    "        # Iterate through all documents and assign based on round-robin splitting\n",
    "        # Documents with cv % num_folds == fold_idx go to test, others to train\n",
    "        for doc in documents:\n",
    "            if doc[\"cv\"] % num_folds == fold_idx:\n",
    "                test_fold.append(doc)\n",
    "            else:\n",
    "                train_fold.append(doc)\n",
    "\n",
    "        folds.append((train_fold, test_fold))\n",
    "\n",
    "    return folds\n",
    "    ##################\n",
    "    # raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbzmBa0In80H"
   },
   "source": [
    "Now re-train the Naive Bayes classifier on each fold, and evaluate on that fold's held-out test set. Make sure to print the accuracy mean and standard deviation.\n",
    "\n",
    "Note: you may use `tqdm` to add progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-RyhL_3n80H"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation: 100%|██████████| 10/10 [00:03<00:00,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "10-Fold Cross-Validation Results:\n",
      "Mean Accuracy: 0.8240\n",
      "Standard Deviation: 0.0282\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "\n",
    "# Get the cross-validation splits\n",
    "cv_splits = cross_validation_splits(reviews, num_folds=10)\n",
    "\n",
    "# Store accuracies for each fold\n",
    "fold_accuracies = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for fold_idx, (train_fold, test_fold) in enumerate(tqdm.tqdm(cv_splits, desc=\"Cross-validation\")):\n",
    "    # Extract labels\n",
    "    fold_train_labels = extract_labels(train_fold)\n",
    "    fold_test_labels = extract_labels(test_fold)\n",
    "\n",
    "    # Extract unigram tokens\n",
    "    fold_train_tokens = extract_unigrams(train_fold)\n",
    "    fold_test_tokens = extract_unigrams(test_fold)\n",
    "\n",
    "    # Initialize and train Naive Bayes classifier with smoothing\n",
    "    nb_classifier_cv = NaiveBayes(smoothing=1.0)\n",
    "    nb_classifier_cv.fit(fold_train_tokens, train_labels)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_fold = nb_classifier_cv.predict(fold_test_tokens)\n",
    "\n",
    "    # Calculate accuracy for this fold\n",
    "    fold_accuracy = accuracy(predictions_fold, fold_test_labels)  # type: ignore\n",
    "    fold_accuracies.append(fold_accuracy)\n",
    "\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean_accuracy_nb = np.mean(fold_accuracies)\n",
    "std_accuracy_nb = np.std(fold_accuracies)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"10-Fold Cross-Validation Results:\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy_nb:.4f}\")\n",
    "print(f\"Standard Deviation: {std_accuracy_nb:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "##################\n",
    "# raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_v-Dl_1Jn80H"
   },
   "source": [
    "**FROM NOW ON, ALWAYS USE CROSS VALIDATION WHEN EVALUATING A CLASSIFIER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6A2zX9_BRKm"
   },
   "source": [
    "## Features, overfitting, and the curse of dimensionality\n",
    "\n",
    "In the Bag-of-Words model, ideally we would like each distinct word in\n",
    "the text to be mapped to its own dimension in the output vector\n",
    "representation. However, real world text is messy, and we need to decide\n",
    "on what we consider to be a word. For example, is “`word`\" different\n",
    "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
    "definition, and the number of features explodes, while our algorithm\n",
    "fails to learn anything generalisable. Too lax, and we risk destroying\n",
    "our learning signal. In the following section, you will learn about\n",
    "confronting the feature sparsity and the overfitting problems as they\n",
    "occur in NLP classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKK8FNt8VtcZ"
   },
   "source": [
    "### Stemming (1.5pts)\n",
    "\n",
    "To make your algorithm more robust, use stemming and\n",
    "hash different inflections of a word to the same feature in the BoW\n",
    "vector space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jpsc1fId-5IE"
   },
   "source": [
    "#### (Q2.6): How does the performance of your classifier change when you use stemming on your training and test datasets? (1pt)\n",
    "\n",
    "First we'll need to adjust our `extract_unigrams` function to extract stemmed unigrams. Use the provided structure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tA_XoWeNn80I"
   },
   "outputs": [],
   "source": [
    "def extract_stems(\n",
    "    documents: list[dict[str, typing.Any]], stemmer: PorterStemmer, lower: bool = True\n",
    ") -> list[list[str]]:\n",
    "    \"\"\"Extract stemmed unigrams from a corpus of documents.\n",
    "\n",
    "    Args:\n",
    "        documents (list[dict[str, Any]]): the list of documents\n",
    "        stemmer (PorterStemmer): the stemmer to use when stemming documents\n",
    "        lower (bool, optional): whether to lowercase tokens before stemming. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[list[str]]: the list of token lists\n",
    "    \"\"\"\n",
    "    ##################\n",
    "\n",
    "    # Initialize list to collect all stemmed tokens from all documents\n",
    "    all_stems = []\n",
    "\n",
    "    for doc in documents:\n",
    "        # Initialize list for stemmed tokens in this specific document\n",
    "        stems_per_doc = []\n",
    "\n",
    "        # Loop over sentences in the document\n",
    "        for sentence in doc[\"content\"]:\n",
    "\n",
    "            # Extract word and POS tag from each token\n",
    "            for word, pos in sentence:\n",
    "                # Lowercase the word if requested\n",
    "                if lower:\n",
    "                    word = word.lower()\n",
    "\n",
    "                # Stem the word using the provided stemmer\n",
    "                stemmed_word = stemmer.stem(word)\n",
    "                stems_per_doc.append(stemmed_word)\n",
    "\n",
    "        all_stems.append(stems_per_doc)\n",
    "\n",
    "    return all_stems\n",
    "\n",
    "    ##################\n",
    "    # raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdYfIJsjn80I"
   },
   "source": [
    "Now re-train your Naive Bayes classifier. Use cross-validation to evaluate the classifier. Use nltk's `PorterStemmer` to stem the document tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wq-X7wokn80I"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation with stemming: 100%|██████████| 10/10 [00:51<00:00,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "10-Fold Cross-Validation Results (with Stemming):\n",
      "Mean Accuracy: 0.8185\n",
      "Standard Deviation: 0.0292\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Store accuracies for each fold\n",
    "fold_accuracies_stemmed = []\n",
    "\n",
    "# Perform 10-fold cross-validation with stemming\n",
    "for fold_idx, (train_fold, test_fold) in enumerate(\n",
    "    tqdm.tqdm(cv_splits, desc=\"Cross-validation with stemming\")\n",
    "):\n",
    "    # Extract labels\n",
    "    fold_train_labels = extract_labels(train_fold)\n",
    "    fold_test_labels = extract_labels(test_fold)\n",
    "\n",
    "    # Extract STEMMED tokens (this is the key difference from Q2.5)\n",
    "    fold_train_stems = extract_stems(train_fold, stemmer=stemmer, lower=True)\n",
    "    fold_test_stems = extract_stems(test_fold, stemmer=stemmer, lower=True)\n",
    "\n",
    "    # Initialize and train Naive Bayes classifier with smoothing\n",
    "    nb_classifier_stem = NaiveBayes(smoothing=1.0)\n",
    "    nb_classifier_stem.fit(fold_train_stems, fold_train_labels)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_fold = nb_classifier_stem.predict(fold_test_stems)\n",
    "\n",
    "    # Calculate accuracy for this fold\n",
    "    fold_accuracy = accuracy(predictions_fold, fold_test_labels)  # type: ignore\n",
    "    fold_accuracies_stemmed.append(fold_accuracy)\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean_accuracy_stemmed = np.mean(fold_accuracies_stemmed)\n",
    "std_accuracy_stemmed = np.std(fold_accuracies_stemmed)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"10-Fold Cross-Validation Results (with Stemming):\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy_stemmed:.4f}\")\n",
    "print(f\"Standard Deviation: {std_accuracy_stemmed:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "##################\n",
    "# raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkDHVq_1XUVP"
   },
   "source": [
    "#### (Q2.7) What happens to the number of features (i.e., the size of the vocabulary) when using stemming as opposed to (Q2.4)? (0.5pt)\n",
    "\n",
    "Give actual numbers. You can use the training set from Q2.4 to determine these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jcPh63Cn80I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size With stemming: 32404\n",
      "Vocabulary size Without stemming: 45348\n",
      "Reduction in features: 12944 (28.54%)\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "\n",
    "# Build vocabulary by collecting all unique tokens from the Q2.4 train tokens\n",
    "vocab_no_stem = set()\n",
    "for doc_tokens in train_tokens:\n",
    "    vocab_no_stem.update(doc_tokens)\n",
    "\n",
    "vocab_size_no_stem = len(vocab_no_stem)\n",
    "\n",
    "# Get vocabulary size with stemming\n",
    "stemmer = PorterStemmer()\n",
    "train_tokens_with_stem = extract_stems(train_docs, stemmer=stemmer, lower=True)\n",
    "\n",
    "# Build vocabulary by collecting all unique stems\n",
    "vocab_with_stem = set()\n",
    "for doc_stems in train_tokens_with_stem:\n",
    "    vocab_with_stem.update(doc_stems)\n",
    "\n",
    "vocab_size_with_stem = len(vocab_with_stem)\n",
    "\n",
    "# Calculate the reduction\n",
    "reduction = vocab_size_no_stem - vocab_size_with_stem\n",
    "reduction_percent = (reduction / vocab_size_no_stem) * 100\n",
    "\n",
    "print(f\"Vocabulary size With stemming: {vocab_size_with_stem}\")\n",
    "print(f\"Vocabulary size Without stemming: {vocab_size_no_stem}\")\n",
    "print(f\"Reduction in features: {reduction} ({reduction_percent:.2f}%)\")\n",
    "\n",
    "##################\n",
    "# raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoazfxbNV5Lq"
   },
   "source": [
    "### N-grams (2.5pts)\n",
    "\n",
    "A simple way of retaining some of the word\n",
    "order information when using bag-of-words representations is to add **n-gram** features.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHjy3I7-qWiu"
   },
   "source": [
    "#### (Q2.8) Retrain your classifier from (Q2.4) using **unigrams+bigrams** and **unigrams+bigrams+trigrams** as features. (2pt)\n",
    "\n",
    "We'll need to adjust our `extract_unigrams` function to extract stemmed unigrams. Use the provided structure below. You can use nltk's `ngrams` function to extract n-grams from the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZyHrHL7n80K"
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(\n",
    "    documents: list[dict], n_values: typing.Sequence[int], lower: bool = True\n",
    ") -> list[list[typing.Any]]:\n",
    "    \"\"\"Extract n-gram features from the corpus of documents.\n",
    "\n",
    "    Args:\n",
    "        documents (list[dict]): the list of documents\n",
    "        n_values (typing.Sequence[int]): the different sizes of n-grams to take (1=unigram, 2=bigram)\n",
    "        lower (bool, optional): whether to lowercase the tokens. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[list[tuple[str]]]: the n-gram tokens\n",
    "    \"\"\"\n",
    "    ##################\n",
    "\n",
    "    # Initialize list to collect all n-grams from all documents\n",
    "    all_ngrams = []\n",
    "\n",
    "    # Iterate over each document\n",
    "    for doc in documents:\n",
    "        # Initialize list for n-grams in this specific document\n",
    "        ngrams_per_doc = []\n",
    "\n",
    "        # Loop over sentences in the document\n",
    "        for sentence in doc[\"content\"]:\n",
    "            # Extract just the words (not POS tags) from the sentence\n",
    "            words = [word for word, _ in sentence]\n",
    "\n",
    "            # Lowercase if requested\n",
    "            if lower:\n",
    "                words = [word.lower() for word in words]\n",
    "\n",
    "            # For each n-gram size requested (e.g., [1, 2] for unigrams and bigrams)\n",
    "            for n in n_values:\n",
    "                # Use nltk's ngrams function to extract n-grams\n",
    "                sentence_ngrams = list(ngrams(words, n))\n",
    "\n",
    "                # Add the n-grams from this sentence to the document's n-grams\n",
    "                ngrams_per_doc.extend(sentence_ngrams)\n",
    "\n",
    "        # Add this document's n-grams to the collection\n",
    "        all_ngrams.append(ngrams_per_doc)\n",
    "\n",
    "    return all_ngrams\n",
    "\n",
    "    ##################\n",
    "    # raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3zo7EVzn80K"
   },
   "source": [
    "Now re-train your Naive Bayes classifier with **uni- and bigram** tokens. Report accuracy and compare it with that of the approaches you have previously implemented. Use cross-validation when evaluating your classifier. (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSlLfRfyn80K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation with unigrams+bigrams: 100%|██████████| 10/10 [00:19<00:00,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "10-Fold Cross-Validation Results (Unigrams + Bigrams):\n",
      "Mean Accuracy: 0.8290\n",
      "Standard Deviation: 0.0286\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "\n",
    "# Store accuracies for each fold\n",
    "fold_accuracies_bigrams = []\n",
    "\n",
    "# Perform 10-fold cross-validation with unigrams + bigrams\n",
    "for fold_idx, (train_fold, test_fold) in enumerate(\n",
    "    tqdm.tqdm(cv_splits, desc=\"Cross-validation with unigrams+bigrams\")\n",
    "):\n",
    "    # Extract labels\n",
    "    fold_train_labels = extract_labels(train_fold)\n",
    "    fold_test_labels = extract_labels(test_fold)\n",
    "\n",
    "    # Extract unigram + bigram features using n_values=[1, 2]\n",
    "    fold_train_ngrams = extract_ngrams(train_fold, n_values=[1, 2], lower=True)\n",
    "    fold_test_ngrams = extract_ngrams(test_fold, n_values=[1, 2], lower=True)\n",
    "\n",
    "    # Initialize and train Naive Bayes classifier with smoothing\n",
    "    nb_classifier_bigrams = NaiveBayes(smoothing=1.0)\n",
    "    nb_classifier_bigrams.fit(fold_train_ngrams, fold_train_labels)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_fold = nb_classifier_bigrams.predict(fold_test_ngrams)\n",
    "\n",
    "    # Calculate accuracy for this fold\n",
    "    fold_accuracy = accuracy(predictions_fold, fold_test_labels)  # type: ignore\n",
    "    fold_accuracies_bigrams.append(fold_accuracy)\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean_accuracy_bigrams = np.mean(fold_accuracies_bigrams)\n",
    "std_accuracy_bigrams = np.std(fold_accuracies_bigrams)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"10-Fold Cross-Validation Results (Unigrams + Bigrams):\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy_bigrams:.4f}\")\n",
    "print(f\"Standard Deviation: {std_accuracy_bigrams:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "\n",
    "##################\n",
    "# raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IB8dHbHn80K"
   },
   "source": [
    "Now re-train your Naive Bayes classifier with **uni-, bi- and trigram** tokens. Report accuracy and compare it with that of the approaches you have previously implemented. Use cross-validation when evaluating your classifier. (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfMvSaxun80K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation with unigrams+bigrams+trigrams: 100%|██████████| 10/10 [00:43<00:00,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "10-Fold Cross-Validation Results (Unigrams + Bigrams + Trigrams):\n",
      "Mean Accuracy: 0.8145\n",
      "Standard Deviation: 0.0288\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Store accuracies for each fold\n",
    "fold_accuracies_trigrams = []\n",
    "\n",
    "# Perform 10-fold cross-validation with unigrams + bigrams + trigrams\n",
    "for fold_idx, (train_fold, test_fold) in enumerate(\n",
    "    tqdm.tqdm(cv_splits, desc=\"Cross-validation with unigrams+bigrams+trigrams\")\n",
    "):\n",
    "    # Extract labels\n",
    "    fold_train_labels = extract_labels(train_fold)\n",
    "    fold_test_labels = extract_labels(test_fold)\n",
    "\n",
    "    # Extract unigram + bigram + trigram features using n_values=[1, 2, 3]\n",
    "    fold_train_ngrams = extract_ngrams(train_fold, n_values=[1, 2, 3], lower=True)\n",
    "    fold_test_ngrams = extract_ngrams(test_fold, n_values=[1, 2, 3], lower=True)\n",
    "\n",
    "    # Initialize and train Naive Bayes classifier with smoothing\n",
    "    nb_classifier_trigrams = NaiveBayes(smoothing=1.0)\n",
    "    nb_classifier_trigrams.fit(fold_train_ngrams, fold_train_labels)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_fold = nb_classifier_trigrams.predict(fold_test_ngrams)\n",
    "\n",
    "    # Calculate accuracy for this fold\n",
    "    fold_accuracy = accuracy(predictions_fold, fold_test_labels)  # type: ignore\n",
    "    fold_accuracies_trigrams.append(fold_accuracy)\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean_accuracy_trigrams = np.mean(fold_accuracies_trigrams)\n",
    "std_accuracy_trigrams = np.std(fold_accuracies_trigrams)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"10-Fold Cross-Validation Results (Unigrams + Bigrams + Trigrams):\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy_trigrams:.4f}\")\n",
    "print(f\"Standard Deviation: {std_accuracy_trigrams:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "##################\n",
    "# raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVrGGArkrWoL"
   },
   "source": [
    "#### (Q2.9): How many features does the BoW model have to take into account now? (0.5pt)\n",
    "How would you expect the number of features to increase theoretically (e.g., linear, square, cubed, exponential)? How do the number of features increase in the held-out training set (compared to Q2.8)? Do you expect this rate of increase to continue for (much) larger n-grams?\n",
    "\n",
    "Use the held-out training set from training set from Q2.4 for this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEGZ9SV8pPaa"
   },
   "source": [
    "\\####################\n",
    "\n",
    "Theoretically we would expect exponentioal increase. For V vocabulary size and n n-gram size each position can have V possibilities. This leads to V^2 bigrams and V^3 trigrams.\n",
    "\n",
    "In practice we see much slower growth due to data sparsity, because many theoretically possible n-gram combinations don't occur in real text. We expect this growth rate to further decrease drastically in larger n-grams sizes for the same reasons.\n",
    "\n",
    "\\####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OiBHIZvn80L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Unigrams: 45,348\n",
      "Unigrams + Bigrams: 465,262 (+925.98%)\n",
      "Uni + Bi + Trigrams: 1,346,107 (+189.32%)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "\n",
    "# Extract different n-gram features from the training set\n",
    "# TODO Question TA on did they mean the training set from Q2.4 as used in previous questions or the test set here (bc. of held-out phrasing)\n",
    "train_unigrams = extract_ngrams(train_docs, n_values=[1], lower=True)\n",
    "train_bigrams = extract_ngrams(train_docs, n_values=[1, 2], lower=True)\n",
    "train_trigrams = extract_ngrams(train_docs, n_values=[1, 2, 3], lower=True)\n",
    "\n",
    "# Build vocabularies\n",
    "vocab_unigrams = set()\n",
    "for doc in train_unigrams:\n",
    "    vocab_unigrams.update(doc)\n",
    "\n",
    "vocab_bigrams = set()\n",
    "for doc in train_bigrams:\n",
    "    vocab_bigrams.update(doc)\n",
    "\n",
    "vocab_trigrams = set()\n",
    "for doc in train_trigrams:\n",
    "    vocab_trigrams.update(doc)\n",
    "\n",
    "# Calculate sizes and growth\n",
    "size_unigrams = len(vocab_unigrams)\n",
    "size_bigrams = len(vocab_bigrams)\n",
    "size_trigrams = len(vocab_trigrams)\n",
    "\n",
    "increase_bigrams = size_bigrams - size_unigrams\n",
    "increase_trigrams = size_trigrams - size_bigrams\n",
    "\n",
    "growth_rate_2gram = increase_bigrams / size_unigrams\n",
    "growth_rate_3gram = increase_trigrams / size_bigrams\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Unigrams: {size_unigrams:,}\")\n",
    "print(f\"Unigrams + Bigrams: {size_bigrams:,} (+{growth_rate_2gram:.2%})\")\n",
    "print(f\"Uni + Bi + Trigrams: {size_trigrams:,} (+{growth_rate_3gram:.2%})\")\n",
    "print(f\"{'='*50}\")\n",
    "##################\n",
    "# raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHWKDL3YV6vh"
   },
   "source": [
    "\n",
    "\n",
    "# (3) Support Vector Machines (4pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJSYhcVaoJGt"
   },
   "source": [
    "Though simple to understand, implement, and debug, one major problem with the Naive Bayes classifier is that its performance deteriorates (becomes skewed) when it is being used with features which are not independent (i.e., are correlated). Another popular classifier that doesn’t assume feature independence is the Support Vector Machine (SVM) classifier.\n",
    "\n",
    "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
    "Other sources for learning SVM:\n",
    "* http://web.mit.edu/zoya/www/SVM.pdf\n",
    "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
    "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LnzNtQBV8gr"
   },
   "source": [
    "#### (Q3.1): Train SVM and compare to Naive Bayes (2pts)\n",
    "\n",
    "Train an SVM classifier (`sklearn.svm.LinearSVC`) using the unigram features collected for Naive Bayes.\n",
    "\n",
    "sklearn's classifiers expect a different data structure than what we've been using so far. Instead of a list of tokens, we'll need to provide a (sparse) [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix). Each row represents a document, and each column represents a token. The value in each cell represents how often a token appears in a document.\n",
    "\n",
    "Define a function below that takes a list of tokens and constructs a document-term matrix. While not mandatory, it's recommended to use scipy's `csr_matrix` to produce a sparse matrix representation. This avoids having to keep many 0 values in memory, and can speed up SVM training.\n",
    "\n",
    "Hint: the documentation on the [`csr_matrix`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) is very helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMe4QQWgn80L"
   },
   "outputs": [],
   "source": [
    "def build_term_document_matrix(\n",
    "    train_features: list[list[str]], test_features: list[list[str]]\n",
    ") -> tuple[np.array, np.array]:\n",
    "    \"\"\"Converts a list of token lists to a document-term matrix.\n",
    "\n",
    "    Args:\n",
    "        train_features (list[list[str]]): the training token lists\n",
    "        test_features (list[list[str]]): the testing token lists\n",
    "\n",
    "    Returns:\n",
    "        tuple[array, array]: a tuple of training and testing DTMs\n",
    "    \"\"\"\n",
    "    ##################\n",
    "\n",
    "    # Build vocabulary from training data\n",
    "    vocab = {}\n",
    "    vocab_index = 0\n",
    "    for doc in train_features:\n",
    "        for token in doc:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = vocab_index\n",
    "                vocab_index += 1\n",
    "\n",
    "    # Build training matrix\n",
    "    train_rows = []\n",
    "    train_cols = []\n",
    "    train_data = []\n",
    "\n",
    "    for doc_idx, doc in enumerate(train_features):\n",
    "        token_counts = Counter(doc)\n",
    "        for token, count in token_counts.items():\n",
    "            if token in vocab:  # Should always be true for train\n",
    "                train_rows.append(doc_idx)\n",
    "                train_cols.append(vocab[token])\n",
    "                train_data.append(count)\n",
    "\n",
    "    train_matrix = csr_matrix(\n",
    "        (train_data, (train_rows, train_cols)), shape=(len(train_features), len(vocab))\n",
    "    )\n",
    "\n",
    "    # Build test matrix using same vocabulary\n",
    "    test_rows = []\n",
    "    test_cols = []\n",
    "    test_data = []\n",
    "\n",
    "    for doc_idx, doc in enumerate(test_features):\n",
    "        token_counts = Counter(doc)\n",
    "        for token, count in token_counts.items():\n",
    "            if token in vocab:  # Only use tokens seen in training\n",
    "                test_rows.append(doc_idx)\n",
    "                test_cols.append(vocab[token])\n",
    "                test_data.append(count)\n",
    "\n",
    "    test_matrix = csr_matrix(\n",
    "        (test_data, (test_rows, test_cols)), shape=(len(test_features), len(vocab))\n",
    "    )\n",
    "\n",
    "    return (train_matrix, test_matrix)\n",
    "\n",
    "    ##################\n",
    "    # raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEaKMjsWn80L"
   },
   "source": [
    "Besides a document term matrix, sklearn also expects the labels to be a list of `int`. Using the structure provide, implement a function to convert the `str` labels to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15Lxwo1xn80M"
   },
   "outputs": [],
   "source": [
    "def convert_labels_to_ints(labels: list[str]) -> list[int]:\n",
    "    \"\"\"Converts a list of \"POS\" or \"NEG\" to 0 or 1.\n",
    "\n",
    "    Args:\n",
    "        labels (list[str]): the list of str labels\n",
    "\n",
    "    Returns:\n",
    "        list[int]: the list of int labels\n",
    "    \"\"\"\n",
    "    ##################\n",
    "\n",
    "    return [1 if label == \"POS\" else 0 for label in labels]\n",
    "\n",
    "    ##################\n",
    "    # raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cpn5Zzcen80M"
   },
   "source": [
    "Now train, predict and evaluate an SVM classifier. Compare the classification performance of the SVM classifier to that of the Naive Bayes classifier with smoothing. Use cross-validation to evaluate the performance of the classifiers.\n",
    "\n",
    "You are not required to perform hyperparameter tuning, but it might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7-T8pVVn80M"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation: SVM vs NB: 100%|██████████| 10/10 [00:12<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "10-Fold Cross-Validation Results (Unigrams):\n",
      "SVM Classifier:\n",
      "\tMean Accuracy: 0.8320\n",
      "\tStd Deviation: 0.0244\n",
      "\n",
      "Naive Bayes Classifier (baseline):\n",
      "\tMean Accuracy: 0.8240\n",
      "\tStd Deviation: 0.0282\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "\n",
    "# Store accuracies for both classifiers across folds\n",
    "fold_accuracies_svm = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for fold_idx, (train_fold, test_fold) in enumerate(\n",
    "    tqdm.tqdm(cv_splits, desc=\"Cross-validation: SVM vs NB\")\n",
    "):\n",
    "    # Extract labels\n",
    "    fold_train_labels = extract_labels(train_fold)\n",
    "    fold_test_labels = extract_labels(test_fold)\n",
    "\n",
    "    # Extract unigram tokens\n",
    "    fold_train_tokens = extract_unigrams(train_fold, lower=True)\n",
    "    fold_test_tokens = extract_unigrams(test_fold, lower=True)\n",
    "\n",
    "    # Build document-term matrices\n",
    "    train_dtm, test_dtm = build_term_document_matrix(fold_train_tokens, fold_test_tokens)\n",
    "\n",
    "    # Convert labels to integers for SVM\n",
    "    train_labels_int = convert_labels_to_ints(fold_train_labels)  # type:ignore\n",
    "    test_labels_int = convert_labels_to_ints(fold_test_labels)  # type:ignore\n",
    "\n",
    "    # Train and evaluate SVM\n",
    "    svm_classifier = sk.svm.LinearSVC(random_state=42, max_iter=10000)  # type:ignore\n",
    "    svm_classifier.fit(train_dtm, train_labels_int)\n",
    "    svm_predictions = svm_classifier.predict(test_dtm)\n",
    "\n",
    "    # Convert predictions back to string labels for accuracy calculation\n",
    "    svm_predictions_str = [\"POS\" if pred == 1 else \"NEG\" for pred in svm_predictions]\n",
    "    svm_accuracy = accuracy(svm_predictions_str, fold_test_labels)  # type:ignore\n",
    "    fold_accuracies_svm.append(svm_accuracy)\n",
    "\n",
    "\n",
    "# Calculate statistics\n",
    "mean_accuracy_svm = np.mean(fold_accuracies_svm)\n",
    "std_accuracy_svm = np.std(fold_accuracies_svm)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"10-Fold Cross-Validation Results (Unigrams):\")\n",
    "print(f\"SVM Classifier:\")\n",
    "print(f\"\\tMean Accuracy: {mean_accuracy_svm:.4f}\")\n",
    "print(f\"\\tStd Deviation: {std_accuracy_svm:.4f}\")\n",
    "print(f\"\\nNaive Bayes Classifier (baseline):\")\n",
    "print(f\"\\tMean Accuracy: {mean_accuracy_nb:.4f}\")\n",
    "print(f\"\\tStd Deviation: {std_accuracy_nb:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "##################\n",
    "# raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifXVWcK0V9qY"
   },
   "source": [
    "### POS disambiguation (2pts)\n",
    "\n",
    "Now add in part-of-speech features. You will find the\n",
    "movie review dataset has already been POS-tagged for you ([here](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf) you find the tagset). Try to\n",
    "replicate the results obtained by Pang et al. (2002).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA3I82o4oWGu"
   },
   "source": [
    "#### (Q3.2) Replace your features with word+POS features, and report performance with the SVM. Use cross-validation to evaluate the classifier and compare the results with (Q3.1). Does part-of-speech information help? Explain why this may be the case. (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAMweWC5n80M"
   },
   "source": [
    "\\####################\n",
    "\\# YOUR ANSWER HERE \\#\n",
    "\\####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTYg1vdJn80M"
   },
   "source": [
    "Once again, we'll need to adjust our `extract_unigrams` function to extract (token, pos) tuples. Use the provided structure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuNw47Ixn80M"
   },
   "outputs": [],
   "source": [
    "def extract_pos_unigrams(documents: list[dict], lower: bool = True) -> list[list[tuple[str, str]]]:\n",
    "    \"\"\"Extracts (token, pos) tuples.\n",
    "\n",
    "    Args:\n",
    "        documents (list[dict]): the list of documents\n",
    "        lower (bool, optional): whether to lowercase words. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[list[tuple[str, str]]]: the list of (token, pos) tuple lists\n",
    "    \"\"\"\n",
    "    ##################\n",
    "\n",
    "    all_pos_unigrams = []\n",
    "\n",
    "    # Iterate over each document\n",
    "    for doc in documents:\n",
    "        # Initialize list for (token, pos) tuples in this specific document\n",
    "        pos_unigrams_per_doc = []\n",
    "\n",
    "        # Loop over sentences in the document\n",
    "        for sentence in doc[\"content\"]:\n",
    "            # Each sentence is already a list of (word, pos) tuples\n",
    "            for word, pos in sentence:\n",
    "                # Lowercase the word if requested\n",
    "                if lower:\n",
    "                    word = word.lower()\n",
    "\n",
    "                # Append the (word, pos) tuple\n",
    "                pos_unigrams_per_doc.append((word, pos))\n",
    "\n",
    "        # Add this document's tuples to the collection\n",
    "        all_pos_unigrams.append(pos_unigrams_per_doc)\n",
    "\n",
    "    return all_pos_unigrams\n",
    "\n",
    "    ##################\n",
    "    # raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyzxPrRdn80N"
   },
   "source": [
    "Now train, predict and evaluate your SVM classifier, and compare to the SVM with only unigram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30L3aOeon80N"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation: SVM with POS features: 100%|██████████| 10/10 [00:22<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "10-Fold Cross-Validation Results:\n",
      "\n",
      "SVM with Word+POS features:\n",
      "\tMean Accuracy: 0.8380\n",
      "\tStd Deviation: 0.0193\n",
      "\n",
      "SVM with Unigrams only (from Q3.1):\n",
      "\tMean Accuracy: 0.8320\n",
      "\tStd Deviation: 0.0244\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "\n",
    "# Store accuracies for SVM with POS features\n",
    "fold_accuracies_svm_pos = []\n",
    "\n",
    "# Perform 10-fold cross-validation with word+POS features\n",
    "for fold_idx, (train_fold, test_fold) in enumerate(\n",
    "    tqdm.tqdm(cv_splits, desc=\"Cross-validation: SVM with POS features\")\n",
    "):\n",
    "    # Extract labels\n",
    "    fold_train_labels = extract_labels(train_fold)\n",
    "    fold_test_labels = extract_labels(test_fold)\n",
    "\n",
    "    # Extract word+POS tuple features\n",
    "    fold_train_pos_tokens = extract_pos_unigrams(train_fold, lower=True)\n",
    "    fold_test_pos_tokens = extract_pos_unigrams(test_fold, lower=True)\n",
    "\n",
    "    # Build document-term matrices\n",
    "    # Note: build_term_document_matrix can handle tuples as tokens\n",
    "    train_dtm, test_dtm = build_term_document_matrix(fold_train_pos_tokens, fold_test_pos_tokens)  # type: ignore\n",
    "\n",
    "    # Convert labels to integers for SVM\n",
    "    train_labels_int = convert_labels_to_ints(fold_train_labels)  # type: ignore\n",
    "    test_labels_int = convert_labels_to_ints(fold_test_labels)  # type: ignore\n",
    "\n",
    "    # Train and evaluate SVM\n",
    "    svm_classifier_pos = sk.svm.LinearSVC(random_state=42, max_iter=10000)  # type: ignore\n",
    "    svm_classifier_pos.fit(train_dtm, train_labels_int)\n",
    "    svm_predictions = svm_classifier_pos.predict(test_dtm)\n",
    "\n",
    "    # Convert predictions back to string labels for accuracy calculation\n",
    "    svm_predictions_str = [\"POS\" if pred == 1 else \"NEG\" for pred in svm_predictions]\n",
    "    svm_pos_accuracy = accuracy(svm_predictions_str, fold_test_labels)  # type: ignore\n",
    "    fold_accuracies_svm_pos.append(svm_pos_accuracy)\n",
    "\n",
    "# Calculate statistics\n",
    "mean_accuracy_svm_pos = np.mean(fold_accuracies_svm_pos)\n",
    "std_accuracy_svm_pos = np.std(fold_accuracies_svm_pos)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"10-Fold Cross-Validation Results:\")\n",
    "print(f\"\\nSVM with Word+POS features:\")\n",
    "print(f\"\\tMean Accuracy: {mean_accuracy_svm_pos:.4f}\")\n",
    "print(f\"\\tStd Deviation: {std_accuracy_svm_pos:.4f}\")\n",
    "print(f\"\\nSVM with Unigrams only (from Q3.1):\")\n",
    "print(f\"\\tMean Accuracy: {mean_accuracy_svm:.4f}\")\n",
    "print(f\"\\tStd Deviation: {std_accuracy_svm:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "##################\n",
    "\n",
    "# raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su-3w87eMW0w"
   },
   "source": [
    "#### (Q3.3) Discard all closed-class words from your data (keep only nouns, verbs, adjectives, and adverbs), and report performance. Does this help? Use cross-validation to evaluate the classifier and compare the results with (Q3.2). Are closed-class words detrimental to the classifier? Explain why this may be the case. (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOx9ULrEn80N"
   },
   "source": [
    "\\####################\n",
    "\n",
    "Removing closed-class words and keeping only open-class words slightly improved classification performance.\n",
    "\n",
    "By removing closed-class words, we reduce the feature space, which helps prevent overfitting. Closed-class words are very frequent but may not consistently correlate with sentiment. Words like \"excellent,\" \"loved,\" or \"disappointed\" directly express sentiment, while function words are likely appear with similar frequencies in both positive and negative reviews.\n",
    "\n",
    "\\####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srbgwrXhn80N"
   },
   "source": [
    "For the final time, we'll need to adjust our `extract_pos_unigrams` function to filter (token, pos) tuples for open-class words. Use the provided structure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5EwGrpon80N"
   },
   "outputs": [],
   "source": [
    "def extract_open_class_pos_unigrams(\n",
    "    documents: list[dict], lower: bool = True\n",
    ") -> list[list[tuple[str, str]]]:\n",
    "    \"\"\"Extracts (token, pos) tuples for open-class words only.\n",
    "\n",
    "    Args:\n",
    "        documents (list[dict]): the list of documents\n",
    "        lower (bool, optional): whether to lowercase words. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[list[tuple[str, str]]]: the list of (token, pos) tuple lists\n",
    "    \"\"\"\n",
    "    ##################\n",
    "\n",
    "    # Define open-class POS tags (nouns, verbs, adjectives, adverbs)\n",
    "    open_class_tags = {\n",
    "        \"NN\",\n",
    "        \"NNS\",\n",
    "        \"NNP\",\n",
    "        \"NNPS\",  # Nouns\n",
    "        \"VB\",\n",
    "        \"VBD\",\n",
    "        \"VBG\",\n",
    "        \"VBN\",\n",
    "        \"VBP\",\n",
    "        \"VBZ\",  # Verbs\n",
    "        \"JJ\",\n",
    "        \"JJR\",\n",
    "        \"JJS\",  # Adjectives\n",
    "        \"RB\",\n",
    "        \"RBR\",\n",
    "        \"RBS\",  # Adverbs\n",
    "    }\n",
    "\n",
    "    all_open_class_unigrams = []\n",
    "\n",
    "    # Iterate over each document\n",
    "    for doc in documents:\n",
    "        # Initialize list for (token, pos) tuples in this specific document\n",
    "        open_class_unigrams_per_doc = []\n",
    "\n",
    "        # Loop over sentences in the document\n",
    "        for sentence in doc[\"content\"]:\n",
    "            # Each sentence is a list of (word, pos) tuples\n",
    "            for word, pos in sentence:\n",
    "                # Only keep tokens with open-class POS tags\n",
    "                if pos in open_class_tags:\n",
    "                    # Lowercase the word if requested\n",
    "                    if lower:\n",
    "                        word = word.lower()\n",
    "\n",
    "                    # Append the (word, pos) tuple\n",
    "                    open_class_unigrams_per_doc.append((word, pos))\n",
    "\n",
    "        # Add this document's tuples to the collection\n",
    "        all_open_class_unigrams.append(open_class_unigrams_per_doc)\n",
    "\n",
    "    return all_open_class_unigrams\n",
    "\n",
    "    ##################\n",
    "    # raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_myXgvvQn80N"
   },
   "source": [
    "Now train, predict and evaluate your SVM classifier, and compare to the SVM with all POS tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCUPlPozCYUX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation: SVM with open-class POS features: 100%|██████████| 10/10 [00:07<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "10-Fold Cross-Validation Results:\n",
      "\n",
      "SVM with Open-Class Word+POS features:\n",
      "\tMean Accuracy: 0.8490\n",
      "\tStd Deviation: 0.0189\n",
      "\n",
      "SVM with All Word+POS features (from Q3.2):\n",
      "\tMean Accuracy: 0.8380\n",
      "\tStd Deviation: 0.0193\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "\n",
    "# Store accuracies for SVM with open-class POS features\n",
    "fold_accuracies_svm_open = []\n",
    "\n",
    "# Perform 10-fold cross-validation with open-class word+POS features\n",
    "for fold_idx, (train_fold, test_fold) in enumerate(\n",
    "    tqdm.tqdm(cv_splits, desc=\"Cross-validation: SVM with open-class POS features\")\n",
    "):\n",
    "    # Extract labels\n",
    "    fold_train_labels = extract_labels(train_fold)\n",
    "    fold_test_labels = extract_labels(test_fold)\n",
    "\n",
    "    # Extract open-class word+POS tuple features\n",
    "    fold_train_open_tokens = extract_open_class_pos_unigrams(train_fold, lower=True)\n",
    "    fold_test_open_tokens = extract_open_class_pos_unigrams(test_fold, lower=True)\n",
    "\n",
    "    # Build document-term matrices\n",
    "    train_dtm, test_dtm = build_term_document_matrix(fold_train_open_tokens, fold_test_open_tokens)  # type: ignore\n",
    "\n",
    "    # Convert labels to integers for SVM\n",
    "    train_labels_int = convert_labels_to_ints(fold_train_labels)  # type: ignore\n",
    "    test_labels_int = convert_labels_to_ints(fold_test_labels)  # type: ignore\n",
    "\n",
    "    # Train and evaluate SVM\n",
    "    svm_classifier_open = sk.svm.LinearSVC(random_state=42, max_iter=10000)  # type: ignore\n",
    "    svm_classifier_open.fit(train_dtm, train_labels_int)\n",
    "    svm_predictions = svm_classifier_open.predict(test_dtm)\n",
    "\n",
    "    # Convert predictions back to string labels for accuracy calculation\n",
    "    svm_predictions_str = [\"POS\" if pred == 1 else \"NEG\" for pred in svm_predictions]\n",
    "    svm_open_accuracy = accuracy(svm_predictions_str, fold_test_labels)  # type: ignore\n",
    "    fold_accuracies_svm_open.append(svm_open_accuracy)\n",
    "\n",
    "# Calculate statistics\n",
    "mean_accuracy_svm_open = np.mean(fold_accuracies_svm_open)\n",
    "std_accuracy_svm_open = np.std(fold_accuracies_svm_open)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"10-Fold Cross-Validation Results:\")\n",
    "print(f\"\\nSVM with Open-Class Word+POS features:\")\n",
    "print(f\"\\tMean Accuracy: {mean_accuracy_svm_open:.4f}\")\n",
    "print(f\"\\tStd Deviation: {std_accuracy_svm_open:.4f}\")\n",
    "print(f\"\\nSVM with All Word+POS features (from Q3.2):\")\n",
    "print(f\"\\tMean Accuracy: {mean_accuracy_svm_pos:.4f}\")\n",
    "print(f\"\\tStd Deviation: {std_accuracy_svm_pos:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "##################\n",
    "\n",
    "# raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfwqOciAl2No"
   },
   "source": [
    "# (4) Discussion (max. 500 words). (5pts)\n",
    "\n",
    "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
    "Why is this important? What are the limitations of these features and techniques?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYuse5WLmekZ"
   },
   "source": [
    "*Write your answer here in up to 500 words (-0.25pt for >50 extra words, -0.5 points for >100 extra words, ...)*.\n",
    "\n",
    "\n",
    "The experiments in this practical reveal several important insights about which features and techniques work best for sentiment analysis in movie reviews. Clear patterns emerged showing what approaches succeed and where traditional NLP classification methods fall short.\n",
    "\n",
    "Machine learning classifiers (Support Vector Machines and Naive Bayes with smoothing) clearly outperformed lexicon-based approaches. While the lexicon method achieved around 69% accuracy, NB and SVM pushed past 80%. This significant jump shows that statistical patterns learned from training data capture sentiment nuances far better than manually created sentiment dictionaries. The lexicon approach falls short because it depends on fixed word polarities, ignoring context, domain-specific meanings, and the intricate ways people express sentiment in everyday language.\n",
    "\n",
    "Different feature engineering techniques showed mixed results. Laplace smoothing turned out to be critical for Naive Bayes, solving the zero-probability issue when the model encounters new words during testing. A smoothing factor of 1.0 struck a good balance between generalization and maintaining learned patterns. Stemming cut vocabulary size by 28% by merging morphological variants like “loved” and “loving” into single stems. This dimensionality reduction helped prevent overfitting with only minimal impact on accuracy, indicating that different word forms typically convey similar sentiment.\n",
    "\n",
    "The n-gram experiments highlighted important compromises. Bigrams and trigrams can capture compositional meaning and things like negation, but they exploded the feature space and created serious data sparsity problems. While feature growth is theoretically exponential with n-gram length, it’s actually slower in practice since many combinations never appear in real text. With limited training data, higher-order n-grams often decreased performance by generating too many rare or one-off features that couldn’t be estimated reliably.\n",
    "Part-of-speech tagging offered useful disambiguation, helping models differentiate between different uses of identical word forms. \n",
    "\n",
    "Filtering down to just open-class words (nouns, verbs, adjectives, adverbs) brought additional modest gains. This approach cuts down noise from common function words that appear at similar rates in both positive and negative reviews, making them weak predictors. Removing them lets the classifier concentrate on features with stronger predictive value.\n",
    "\n",
    "The biggest limitation across all methods is the bag-of-words representation, which neglects word order beyond local n-grams and misses long-distance dependencies, discourse structure, and nuanced phenomena like sarcasm and irony. Cross-validation consistently demonstrated that matching feature complexity to available training data is essential. Dimensionality remains a core challenge as more sophisticated features demand exponentially more training examples to estimate reliably and avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4hweKFFc1gA"
   },
   "source": [
    "# Use of AI tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a21aQFAVc33O"
   },
   "source": [
    "By submitting this notebook for grading you testify that:\n",
    "* AI did not draft an earlier version of your work.\n",
    "* You did not use AI-powered code completion.\n",
    "* You did not implement algorithms suggested by an AI tool.\n",
    "* AI did not revise a version of your work.\n",
    "* You did not implement suggestions made by an AI tool.\n",
    "\n",
    "You in the sentences above refers to you and your team member(s). AI refers to LM-based tools and assistants (e.g. ChatGPT, UvA AI Chat, Gemini, etc.).\n",
    "\n",
    "If you did make use of an AI tool, you should describe the uses you made of it below. Or indicate that no such tool was used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-jKEfPuc50G"
   },
   "source": [
    "\\####################\n",
    "\\# **YOUR ANSWER HERE** \\#\n",
    "\\####################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "text-analysis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
